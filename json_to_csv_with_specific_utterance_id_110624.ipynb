{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3118087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "def extract_context_sentences(utterances, target_index, context_size=5):\n",
    "    context_start_index = max(0, target_index - context_size)\n",
    "    context_end_index = min(len(utterances), target_index + context_size + 1)\n",
    "    context_sentences = utterances[context_start_index:context_end_index]\n",
    "    return context_sentences\n",
    "\n",
    "def extract_metadata_titles(folder_path):\n",
    "    metadata_titles_with_caret = []\n",
    "    metadata_titles_data = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        if filename.endswith('.json'):\n",
    "            with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "                data = json.load(json_file)\n",
    "                document = data.get('document', [{}])[0]\n",
    "                utterances = document.get('utterance', [])\n",
    "                for i, utterance in enumerate(utterances):\n",
    "                    utterance_form = utterance.get('form', '')\n",
    "                    if '^^' in utterance_form:\n",
    "                        metadata_title = data.get('metadata', {}).get('title', '')\n",
    "                        metadata_titles_with_caret.append((filename, metadata_title, i, utterances))\n",
    "                        metadata_titles_data.append(data)\n",
    "                        \n",
    "    return metadata_titles_with_caret, metadata_titles_data\n",
    "\n",
    "def create_csv_with_context_sentences(folder_path, metadata_titles_with_caret, metadata_titles_data, csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['setting_relation', 'setting_intimacy', 'setting_contact_frequency', 'document_topic', 'sentence_utterance_id', 'speaker_id', 'utterance_form', 'sentence_time', 'speaker_age', 'speaker_occupation', 'speaker_sex'])\n",
    "        \n",
    "        for (filename, metadata_title, target_index, utterances), data in zip(metadata_titles_with_caret, metadata_titles_data):\n",
    "            document = data.get('document', [{}])[0]\n",
    "            document_title = document.get('metadata', {}).get('title', '')\n",
    "            document_topic = document.get('metadata', {}).get('topic', '')\n",
    "            setting_relation = document.get('metadata', {}).get('setting', {}).get('relation', '')\n",
    "            setting_intimacy = document.get('metadata', {}).get('setting', {}).get('intimacy', '')\n",
    "            setting_contact_frequency = document.get('metadata', {}).get('setting', {}).get('contact_frequency', '')\n",
    "            target_utterance = utterances[target_index]\n",
    "            utterance_form = target_utterance.get('form', '')\n",
    "            utterance_id = target_utterance.get('id', '')\n",
    "            speaker_id = target_utterance.get('speaker_id', '')\n",
    "            speakers = document.get('metadata', {}).get('speaker', [])\n",
    "            speaker_info = next((speaker for speaker in speakers if speaker.get('id') == speaker_id), {})\n",
    "            speaker_age = speaker_info.get('age', '')\n",
    "            speaker_occupation = speaker_info.get('occupation', '')\n",
    "            speaker_sex = speaker_info.get('sex', '')\n",
    "            speaker_birthplace = speaker_info.get('birthplace', '')\n",
    "            speaker_principal_residence = speaker_info.get('pricipal_residence', '')\n",
    "            speaker_current_residence = speaker_info.get('current_residence', '')\n",
    "            speaker_device = speaker_info.get('device', '')\n",
    "            speaker_keyboard = speaker_info.get('keyboard', '')\n",
    "            context_sentences = extract_context_sentences(utterances, target_index)\n",
    "            \n",
    "            for sentence in context_sentences:\n",
    "                sentence_utterance_id = sentence.get('id', '')  # 수정된 부분: 각 context_sentence의 원래 utterance_id\n",
    "                sentence_time = sentence.get('time', '')\n",
    "                speaker_id = sentence.get('speaker_id', '')\n",
    "                speaker_info = next((speaker for speaker in speakers if speaker.get('id') == speaker_id), {})\n",
    "                speaker_age = speaker_info.get('age', '')\n",
    "                speaker_occupation = speaker_info.get('occupation', '')\n",
    "                speaker_sex = speaker_info.get('sex', '')\n",
    "                speaker_birthplace = speaker_info.get('birthplace', '')\n",
    "                speaker_principal_residence = speaker_info.get('pricipal_residence', '')\n",
    "                speaker_current_residence = speaker_info.get('current_residence', '')\n",
    "                speaker_device = speaker_info.get('device', '')\n",
    "                speaker_keyboard = speaker_info.get('keyboard', '')\n",
    "                writer.writerow([setting_relation, setting_intimacy, setting_contact_frequency, document_topic, sentence_utterance_id, speaker_id, sentence.get('form', ''), sentence_time, speaker_age, speaker_occupation, speaker_sex])\n",
    "                \n",
    "                \n",
    "# JSON 파일이 있는 폴더 경로 설정\n",
    "folder_path = \"C:\\\\Users\\\\ArumPark\\\\Desktop\\\\Projekt_Hamburg\\\\NIKL_MESSENGER_v2.0\\\\국립국어원 메신저 말뭉치(버전 2.0)\"\n",
    "\n",
    "# '^^'를 포함하는 utterance_form을 가진 metadata_title을 추출합니다.\n",
    "metadata_titles_with_caret, metadata_titles_data = extract_metadata_titles(folder_path)  # 수정된 부분\n",
    "\n",
    "# CSV 파일 경로 설정\n",
    "csv_file_path = './output2.csv'\n",
    "\n",
    "# '^^'를 포함하는 utterance_form을 가진 metadata_title을 가진 JSON 파일만을 선택하여 CSV 파일로 생성합니다.\n",
    "create_csv_with_context_sentences(folder_path, metadata_titles_with_caret, metadata_titles_data, csv_file_path)  # 수정된 부분\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f59c195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def extract_context_sentences(utterances, target_index, context_size=5):\n",
    "    context_start_index = max(0, target_index - context_size)\n",
    "    context_end_index = min(len(utterances), target_index + context_size + 1)\n",
    "    context_sentences = utterances[context_start_index:context_end_index]\n",
    "    return context_sentences\n",
    "\n",
    "def extract_metadata_titles(folder_path, target_utterance_ids):\n",
    "    metadata_titles_with_caret = []\n",
    "    metadata_titles_data = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        if filename.endswith('.json'):\n",
    "            with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "                data = json.load(json_file)\n",
    "                document = data.get('document', [{}])[0]\n",
    "                utterances = document.get('utterance', [])\n",
    "                for i, utterance in enumerate(utterances):\n",
    "                    utterance_id = utterance.get('id', '')\n",
    "                    if utterance_id in target_utterance_ids:\n",
    "                        metadata_title = data.get('metadata', {}).get('title', '')\n",
    "                        metadata_titles_with_caret.append((filename, metadata_title, i, utterances))\n",
    "                        metadata_titles_data.append(data)\n",
    "                        \n",
    "    return metadata_titles_with_caret, metadata_titles_data\n",
    "\n",
    "def create_csv_with_context_sentences(folder_path, metadata_titles_with_caret, metadata_titles_data, csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['setting_relation', 'setting_intimacy', 'setting_contact_frequency', 'document_topic', 'sentence_utterance_id', 'speaker_id', 'utterance_form', 'sentence_time', 'speaker_age', 'speaker_occupation', 'speaker_sex', 'speaker_birthplace', 'speaker_principal_residence', 'speaker_current_residence', 'speaker_device', 'speaker_keyboard'])\n",
    "        \n",
    "        for (filename, metadata_title, target_index, utterances), data in zip(metadata_titles_with_caret, metadata_titles_data):\n",
    "            document = data.get('document', [{}])[0]\n",
    "            document_topic = document.get('metadata', {}).get('topic', '')\n",
    "            setting_relation = document.get('metadata', {}).get('setting', {}).get('relation', '')\n",
    "            setting_intimacy = document.get('metadata', {}).get('setting', {}).get('intimacy', '')\n",
    "            setting_contact_frequency = document.get('metadata', {}).get('setting', {}).get('contact_frequency', '')\n",
    "            target_utterance = utterances[target_index]\n",
    "            utterance_form = target_utterance.get('form', '')\n",
    "            utterance_id = target_utterance.get('id', '')\n",
    "            speaker_id = target_utterance.get('speaker_id', '')\n",
    "            speakers = document.get('metadata', {}).get('speaker', [])\n",
    "            speaker_info = next((speaker for speaker in speakers if speaker.get('id') == speaker_id), {})\n",
    "            speaker_age = speaker_info.get('age', '')\n",
    "            speaker_occupation = speaker_info.get('occupation', '')\n",
    "            speaker_sex = speaker_info.get('sex', '')\n",
    "            speaker_birthplace = speaker_info.get('birthplace', '')\n",
    "            speaker_principal_residence = speaker_info.get('pricipal_residence', '')\n",
    "            speaker_current_residence = speaker_info.get('current_residence', '')\n",
    "            speaker_device = speaker_info.get('device', '')\n",
    "            speaker_keyboard = speaker_info.get('keyboard', '')\n",
    "            context_sentences = extract_context_sentences(utterances, target_index)\n",
    "            \n",
    "            for sentence in context_sentences:\n",
    "                sentence_utterance_id = sentence.get('id', '')\n",
    "                sentence_time = sentence.get('time', '')\n",
    "                speaker_id = sentence.get('speaker_id', '')\n",
    "                speaker_info = next((speaker for speaker in speakers if speaker.get('id') == speaker_id), {})\n",
    "                speaker_age = speaker_info.get('age', '')\n",
    "                speaker_occupation = speaker_info.get('occupation', '')\n",
    "                speaker_sex = speaker_info.get('sex', '')\n",
    "                speaker_birthplace = speaker_info.get('birthplace', '')\n",
    "                speaker_principal_residence = speaker_info.get('pricipal_residence', '')\n",
    "                speaker_current_residence = speaker_info.get('current_residence', '')\n",
    "                speaker_device = speaker_info.get('device', '')\n",
    "                speaker_keyboard = speaker_info.get('keyboard', '')\n",
    "                writer.writerow([setting_relation, setting_intimacy, setting_contact_frequency, document_topic, sentence_utterance_id, speaker_id, sentence.get('form', ''), sentence_time, speaker_age, speaker_occupation, speaker_sex, speaker_birthplace, speaker_principal_residence, speaker_current_residence, speaker_device, speaker_keyboard])\n",
    "\n",
    "# JSON 파일이 있는 폴더 경로 설정\n",
    "folder_path = \"C:\\\\Users\\\\ArumPark\\\\Desktop\\\\Projekt_Hamburg\\\\NIKL_MESSENGER_v2.0\\\\국립국어원 메신저 말뭉치(버전 2.0)\"\n",
    "\n",
    "def get_utterance_ids(file_path, sheet_name, column_name):\n",
    "    try:\n",
    "        # Excel 파일 불러오기\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "        \n",
    "        # 'utterance_id' 열의 값을 리스트로 저장\n",
    "        utterance_ids = df[column_name].tolist()\n",
    "        \n",
    "        return utterance_ids\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return []\n",
    "\n",
    "# 파일 경로와 열 이름을 적절히 수정하여 사용하세요\n",
    "file_path = \"smiliey_new_annotation_file_20chats_each_110624.xlsx\"\n",
    "sheet_name = \"annotation\"\n",
    "column_name = \"utterance_id\"\n",
    "\n",
    "utterance_ids = get_utterance_ids(file_path, sheet_name, column_name)\n",
    "\n",
    "# 추출할 utterance_id 리스트\n",
    "target_utterance_ids = utterance_ids  # 필요한 utterance_id를 여기에 추가\n",
    "\n",
    "# 특정 utterance_id를 포함하는 metadata_title을 추출합니다.\n",
    "metadata_titles_with_caret, metadata_titles_data = extract_metadata_titles(folder_path, target_utterance_ids)\n",
    "\n",
    "# CSV 파일 경로 설정\n",
    "csv_file_path = 'C:/Users/ArumPark/Desktop/Disputation/emoticon/data/new_annotation_context_test3.csv'\n",
    "\n",
    "# 특정 utterance_id를 포함하는 metadata_title을 가진 JSON 파일만을 선택하여 CSV 파일로 생성합니다.\n",
    "create_csv_with_context_sentences(folder_path, metadata_titles_with_caret, metadata_titles_data, csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9faf921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_size 제한하지 않기\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def extract_metadata_titles(folder_path, target_utterance_ids):\n",
    "    metadata_titles_with_caret = []\n",
    "    metadata_titles_data = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        if filename.endswith('.json'):\n",
    "            with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "                data = json.load(json_file)\n",
    "                document = data.get('document', [{}])[0]\n",
    "                utterances = document.get('utterance', [])\n",
    "                for i, utterance in enumerate(utterances):\n",
    "                    utterance_id = utterance.get('id', '')\n",
    "                    if utterance_id in target_utterance_ids:\n",
    "                        metadata_title = data.get('metadata', {}).get('title', '')\n",
    "                        metadata_titles_with_caret.append((filename, metadata_title, i, utterances))\n",
    "                        metadata_titles_data.append(data)\n",
    "                        \n",
    "    return metadata_titles_with_caret, metadata_titles_data\n",
    "\n",
    "def create_excel_with_all_sentences(folder_path, metadata_titles_with_caret, metadata_titles_data, excel_file):\n",
    "    # DataFrame 생성\n",
    "    rows = []\n",
    "    for (filename, metadata_title, _, utterances), data in zip(metadata_titles_with_caret, metadata_titles_data):\n",
    "        document = data.get('document', [{}])[0]\n",
    "        document_topic = document.get('metadata', {}).get('topic', '')\n",
    "        setting_relation = document.get('metadata', {}).get('setting', {}).get('relation', '')\n",
    "        setting_intimacy = document.get('metadata', {}).get('setting', {}).get('intimacy', '')\n",
    "        setting_contact_frequency = document.get('metadata', {}).get('setting', {}).get('contact_frequency', '')\n",
    "        speakers = document.get('metadata', {}).get('speaker', [])\n",
    "        \n",
    "        for sentence in utterances:\n",
    "            sentence_utterance_id = sentence.get('id', '')\n",
    "            sentence_time = sentence.get('time', '')\n",
    "            speaker_id = sentence.get('speaker_id', '')\n",
    "            speaker_info = next((speaker for speaker in speakers if speaker.get('id') == speaker_id), {})\n",
    "            speaker_age = speaker_info.get('age', '')\n",
    "            speaker_occupation = speaker_info.get('occupation', '')\n",
    "            speaker_sex = speaker_info.get('sex', '')\n",
    "            speaker_birthplace = speaker_info.get('birthplace', '')\n",
    "            speaker_principal_residence = speaker_info.get('pricipal_residence', '')\n",
    "            speaker_current_residence = speaker_info.get('current_residence', '')\n",
    "            speaker_device = speaker_info.get('device', '')\n",
    "            speaker_keyboard = speaker_info.get('keyboard', '')\n",
    "            form = sentence.get('form', '')\n",
    "            rows.append([setting_relation, setting_intimacy, setting_contact_frequency, document_topic, sentence_utterance_id, speaker_id, form, sentence_time, speaker_age, speaker_occupation, speaker_sex, speaker_birthplace, speaker_principal_residence, speaker_current_residence, speaker_device, speaker_keyboard])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=['setting_relation', 'setting_intimacy', 'setting_contact_frequency', 'document_topic', 'sentence_utterance_id', 'speaker_id', 'utterance_form', 'sentence_time', 'speaker_age', 'speaker_occupation', 'speaker_sex', 'speaker_birthplace', 'speaker_principal_residence', 'speaker_current_residence', 'speaker_device', 'speaker_keyboard'])\n",
    "\n",
    "    # Excel 파일로 저장\n",
    "    df.to_excel(excel_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# JSON 파일이 있는 폴더 경로 설정\n",
    "folder_path = \"C:\\\\Users\\\\ArumPark\\\\Desktop\\\\Projekt_Hamburg\\\\NIKL_MESSENGER_v2.0\\\\국립국어원 메신저 말뭉치(버전 2.0)\"\n",
    "\n",
    "def get_utterance_ids(file_path, sheet_name, column_name):\n",
    "    try:\n",
    "        # Excel 파일 불러오기\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "        \n",
    "        # 'utterance_id' 열의 값을 리스트로 저장\n",
    "        utterance_ids = df[column_name].tolist()\n",
    "        \n",
    "        return utterance_ids\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return []\n",
    "\n",
    "# 파일 경로와 열 이름을 적절히 수정하여 사용하세요\n",
    "file_path = \"smiliey_new_annotation_file_20chats_each_110624.xlsx\"\n",
    "sheet_name = \"annotation\"\n",
    "column_name = \"utterance_id\"\n",
    "\n",
    "utterance_ids = get_utterance_ids(file_path, sheet_name, column_name)\n",
    "\n",
    "# 추출할 utterance_id 리스트\n",
    "target_utterance_ids = utterance_ids  # 필요한 utterance_id를 여기에 추가\n",
    "\n",
    "# 특정 utterance_id를 포함하는 metadata_title을 추출합니다.\n",
    "metadata_titles_with_caret, metadata_titles_data = extract_metadata_titles(folder_path, target_utterance_ids)\n",
    "\n",
    "# Excel 파일 경로 설정\n",
    "excel_file_path = 'C:/Users/ArumPark/Desktop/Disputation/emoticon/data/new_annotation_context_all_sentences.xlsx'\n",
    "\n",
    "# 특정 utterance_id를 포함하는 metadata_title을 가진 JSON 파일만을 선택하여 Excel 파일로 생성합니다.\n",
    "create_excel_with_all_sentences(folder_path, metadata_titles_with_caret, metadata_titles_data, excel_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91dfc937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output을 excel 파일로\n",
    "\n",
    "# CSV 파일 경로 설정\n",
    "csv_file_path = \"C:/Users/ArumPark/Desktop/Disputation/emoticon/data/new_annotation_context_test2.csv\"\n",
    "\n",
    "# Excel 파일 경로 설정\n",
    "excel_file_path = \"C:/Users/ArumPark/Desktop/Disputation/emoticon/data/new_annotation_context_excel_test.xlsx\"\n",
    "\n",
    "# CSV 파일 불러오기\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Excel 파일로 저장\n",
    "df.to_excel(excel_file_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
